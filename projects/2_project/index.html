<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> project 2 | Melvin Sevi </title> <meta name="author" content="Melvin Sevi"> <meta name="description" content="a project with a background image and giscus comments"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://melvinsevi.github.io/projects/2_project/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Melvin Sevi </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">My Projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/cv/">cv</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">project 2</h1> <p class="post-description">a project with a background image and giscus comments</p> </header> <article> <h1 id="project-page">Project Page</h1> <h2 id="introduction">Introduction</h2> <p>In this project, we aim to improve the performance of the Visual Phrase Grounding (VPD) model proposed by Zha and Rao (Year), focusing particularly on enhancing attention maps for better segmentation accuracy. The VPD model combines vision and language to localize objects referred to by natural language expressions within an image. By refining the attention mechanism of the model, we aim to achieve more accurate and robust object localization.</p> <h2 id="results">Results</h2> <h3 id="segmentation-mask-comparison">Segmentation Mask Comparison</h3> <p><img src="no_mistake/input_image739.png" alt="Segmentation Mask Comparison"> <em>Blurry click down about an inch from top right corner</em></p> <p><img src="no_mistake/input_image745.png" alt="Segmentation Mask Comparison"> <em>Adult to left of kid</em></p> <h3 id="attention-map-analysis">Attention Map Analysis</h3> <p><img src="no_mistake/input_image87.png" alt="Attention Map Analysis"> <em>The fully visible zebra</em></p> <h3 id="segmentation-mask-evaluation">Segmentation Mask Evaluation</h3> <p><img src="no_mistake/input_image87.png" alt="Segmentation Mask Evaluation"> <em>The fully visible zebra</em></p> <h2 id="model-training">Model Training</h2> <h3 id="training-setup-and-results">Training Setup and Results</h3> <p>To assess the computational and qualitative efficiency of retraining the entire SD, which gather near 900M parameters and is initially trained on a single high-end NVIDIA A100 GPU (3$/hour on G-Cloud), we aimed to unlock its full potential while making it feasible to train on commonly available GPU on G-Cloud. Our strategy involved significantly reducing the number of trainable parameters. Concretely, we froze the SD U-Net and disabled the text adapter, thereby shrinking the parameter count from 899M to 39M. Despite this decrease, training continued to be computationally intensive even on top Nvidia GPUs like the T4, P100, or L4 also due to insufficient VRAM memory (unfeasible to obtain multiple GPU machines promptly).</p> <p>To enhance training optimization, we chose a more manageable subset consisting of 3,000 referring expressions from the training set of RefCOCO with 1000 expressions in the validation set. The split is done by taking always the first images in the original split. By employing this split with a diminished batch size of 4 (32 in the paper), we completed training over two epochs using an identical initial learning rate of 0.005 and weight decay value of 0.01 mentioned in the original code. We also used Adam optimizer and cross entropy loss for training. Additionally, we attempted a complete dataset training run with a batch size of 4 (totaling 14 hours); however, the outcomes were poor (see Fig. \ref{tab:mytable3}), likely attributed to the small batch size and quantity of epochs.</p> <p>Experimental evidence suggests that enhancing the model through attention map is beneficial. So our real goal here is to provide the best attention maps to the “classifier”. The problem is that implementing direct cross-attention between CLIP text features and the U-Net feature maps has shown less good performance. The removal of the adapter seems to only sacrifices flexibility without substantial improvement. Using only CLIP is clearly not enough to directly get good segmentation in the attention maps. Indeed these attention maps in the latent space gets better during the generation process of an image by SD (Zhang et al., 2023) but directly trying to get the attention maps from the cross attention between the caption tokens and the feature maps from the U-Net does not give the same results (see Tab. 2). We propose new ideas to address this problem at the end of the paper.</p> <h3 id="model-training-and-use-of-different-noise-scale">Model Training and Use of Different Noise Scale</h3> <p>The (VPD) model demonstrates remarkable performance, achieving 73.56 oIoU on the validation dataset of RefCoCO. However, when freezing SD parameters and removing the Adapter during training, the results are noticeably inferior. In pursuit of improved robustness, another approach explored varying levels of noise applied to the input images both through training and inference. To evaluate the impact of diverse noise scales, we compared attention maps generated across various timestamps during inference of the original VPD. Unfortunately, upon completing the training phase of the freezed SD VPD (F-VPD), the model’s performance on the validation set remains disappointing (see Tab. 3), rendering attention map visualizations unnecessary. Interestingly, during inference, attention maps remain good (see Fig. 7) even under significant noise conditions up to a noise scale factor of 500. As a result, we hypothesize that either the latent representations of clean and noisy images retain considerable shared information, or the U-Net produces similar feature maps regardless of the imposed noise level. Ultimately, the objective involves obtaining optimal segmentations via enhanced attention map so we give insight in the last part on ideas to use different noise scale to obtain better attention maps.</p> <h2 id="further-research-approach">Further research approach:</h2> <ol> <li>Utilizing High-Resolution Feature Maps: Our current implementation uses 32x32 and 16x16 resolution attention maps fed into the classifier. However, increasing the resolution to 64x64 may yield improved results by providing more detailed spatial information about objects within the image.</li> <li>Fine-Tuning Using Low Rank Adaptation Matrices (Hu et al., 2023): To avoid re-training the entire model, fine-tuning specific components can result in improved performance while reducing the required computation. By adjusting the different Q, K, and V weight matrices responsible for cross-attention between textual features and U-Net feature maps, lower rank matrices can be added to maintain information from pre-trained models without training numerous new parameters.</li> <li>Obtaining Higher Resolution Attention Maps via Upsampling: Inspired by the (DAAM) paper (Tang et al., 2023), upsampling attention maps could deliver finer attention details and produce higher resolution output suitable for feeding into the subsequent classification module.</li> <li>Generative Conditioned Denoising through Referring Captions: Adopting a generative approach to obtain better segmentation attention maps may be beneficial. Specifically, adding noise to the image and guiding its denoising based on the provided referring captions could potentially refine attention mechanisms and offer enhanced accuracy.</li> </ol> <h2 id="conclusion">Conclusion</h2> <p>In this study, we aimed to improve the VPD model by Zha and Rao, focusing on enhancing attention maps for better performance. We initially attempted to simplify the model and fully exploit self-distillation, discovering the necessity of using an adapter for effective text embedding refinement. We also explored the impact of different noise scales as inputs, finding the VPD’s robustness to higher noise scales and sensitivity to attention map quality. Unfortunately, due to computational limitations, we couldn’t achieve conclusive results. Nonetheless, our findings provide valuable insights, suggesting promising directions for future research to refine the VPD model.</p> <p><strong>References</strong></p> <ul> <li>Hu, Y., et al. (2023). <em>Title of Hu et al.’s Paper</em>. Journal Name.</li> <li>Tang, Z., et al. (2023). <em>Title of Tang et al.’s Paper</em>. Journal Name.</li> <li>Zhang, X., et al. (2023). <em>Title of Zhang et al.’s Paper</em>. Journal Name.</li> </ul> </article> <div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"melvinsevi/melvinsevi.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Melvin Sevi. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>